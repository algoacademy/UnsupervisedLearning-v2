---
title: "Unsupervised Learning Quiz"
author: "Team Algoritma"
date: "`r format = Sys.Date('%e, %B %Y')`"
output: 
  html_document:
    toc: true
    toc_float:
      collapsed: false
    theme: united
    highlight: zenburn
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(FactoMineR)
library(factoextra)
```

This quiz is part of Algoritma Academy assessment process. Congratulations on completing the Unsupervised Learning course! We will conduct an assessment quiz to test practical unsupervised learning techniques you have learned on the course. The quiz is expected to be taken in the classroom, please contact our team of instructors if you missed the chance to take it in class.

# Data Exploration

In this quiz, we will be using **Fruits-Nutritional Values**. You can get the csv file stored within the folder under `fruits.csv` file. Before we perform data clustering and principle component analysis, we will need to perform exploratory data analysis. You can glimpse the structure of our dataset! You can choose either `str()` or `glimpse()` function.

```{r}
# your code here

```

This dataset consisted of 21 variables with 52 rows of Nutritional values of fruits. The variable contains the name of fruits and another variable present the composition of an element in fruits

Variable `name` represent the name of fruits. Therefore, it would be wise to assign the values of column `name` into row names. We also look at the number of observations containing missing values across observations, if the number of missing values less than 10% from observation, let's take out the observation with missing values.

```{r}
# your code here

```


# 1. Principal Component Analysis (PCA)

## Data Pre-Processing

PCA is very useful to retain information while reducing the dimension of the data. However, we need to make sure that our data is properly scaled in order to get a useful PCA. You may use `scale()` function to scale the numeric variable and store it as `fruits_scale`.

```{r}
# your code here

```

## Build your Principal Component

We have prepared the scaled data to be used for PCA. Next, we will try to generate the principal component from the `fruits_scale`. Recall how we use `FactoMineR` library to perform PCA. Use `PCA()` function from the library to generate a PCA and store it as `pca_fruits`. Remember to set the `scale.unit` parameter to `FALSE` since we already scaled the data manually. Check the summary of the `pca_fruits`.

```{r}
# your code here

```

1. Based on the summary, how many  Principal Components (PCs) will you use if you only tolerate no more than 20% of information loss?
 - [ ] 7 PC (PC 1 through 7)
 - [ ] 5 PC (PC 1 through 5)
 - [ ] 3 PC (PC 1 through 3)
 - [ ] 1 PC (PC 1 only)

Another great implementation of PCA is to visualize high dimensional data into 2 dimensional plot for various purposes, such as cluster analysis or detecting outliers. In order to visualize the PCA, use `plot.PCA()` and function to the `pca_fruits` and then show 2 element that have highest contribution. This will generate an individual PCA plot.

```{r}
# your code here

```

2. Judging from the plot that you have created, which fruit is considered as outlier?
 - [ ] Avocado, Dates deglet noor
 - [ ] Avocado, Lime
 - [ ] Kumquat, Pear
 - [ ] Guava, Pear

We can also create a variable PCA plot that shows the variable loading information of the PCA by simply add `choix = "var"` in the `plot.PCA()`. The loading information will be represented by the length of the arrow from the center of coordinates. The longer the arrow, the bigger loading information of those variable. However, this may not an efficient method if we have a lot of features. Some variables would overlap with each other, making it harder to see the variable names.

An alternative way to extract the loading information is by using the `dimdesc()` function to the `pca_fruits`. Store the result as `pca_dimdesc`. Inspect the loading information from the first dimension/PC by calling `pca_dimdesc$Dim.1` since the first dimension is the one that hold the most information.

```{r}
# your code here

```

3. Mention 3 most contributing variables on PC 1 based on the correlation between variables with the PC 1.
 - [ ] Potassium, Phosphorus, Vitamin B2
 - [ ] Magnessium, Vitamin B3, Fiber
 - [ ] Magnessium, Potassium, Phosphorus
 - [ ] Water, Vitamin E, Fiber
 - [ ] Acidity, Body, Balance

In the principal component analysis, each produced PC has an eigen value obtained from the covariance matrix. The greater the eigen value, the greater the variance captured by the PC.

4. Which of the following is **NOT TRUE** about PCA?
 - [ ] PCA requires data to be scaled so they have the same range of measurement
 - [ ] A Principal Component with an eigenvalue of 0.6 is not more helpful than a PC with an eigenvalue of 6.0
 - [ ] We cannot fully reconstruct the original data from a PCA even when we have eigen value and eigen vector

# 2. K-Means Clustering

Data clustering is a common data mining technique to create clusters of data that can be identified as "data with the same characteristics". Before performing data clustering, you will need to remove the identified *outliers* based on the previous individual PCA plot. The observation for `Avocado` and `Dates, deglet noor` are fairly extending outlier compared to the rest of the observation. Remove these observations from our initial dataset and once again scale the data. 

*Note: You may want to store the scaled data in a new object for we will still need the original data for the cluster profiling analysis.*

```{r}
# your code here

```

## 2.1 Choosing Optimum K

The next step in building a K-means clustering is to find the optimum cluster number to model our data. Use the defined `kmeansTunning()` function below to find the optimum K using Elbow method. Use a maximum of `maxK` as 5 to limit the plot into 5 distinct clusters.

```{r message=F, warning=F}
RNGkind(sample.kind = "Rounding")
kmeansTunning <- function(data, maxK) {
  withinall <- NULL
  total_k <- NULL
  for (i in 1:maxK) {
    set.seed(101)
    temp <- kmeans(data,i)$tot.withinss
    withinall <- append(withinall, temp)
    total_k <- append(total_k,i)
  }
  plot(x = total_k, y = withinall, type = "o", xlab = "Number of Cluster", ylab = "Total within")
}

# kmeansTunning(your_data, maxK = 5)
```

We will select the point where the WSS reduction is no longer significant in (an elbow). Based on the elbow plot, we will choosing k = 2.

K-means is a clustering algorithm that groups the data based on distance. The resulting clusters are stated to be optimum if the distance between data in the same cluster is low and the distance between data from different clusters is high.

5. Which of the following is **NOT TRUE** about choosing the optimal number of clusters?
  - [ ] A high number of clusters indicates a good clustering
  - [ ] Choosing the number of clusters can be based on a business perspective
  - [ ] The location of an *elbow* in Elbow Curve is generally considered as an indicator of the appropriate number of clusters
  - [ ] Elbow method compares the total within sum of square variation from different values of k 

6. Which of the following is **NOT TRUE** about K-Means?
  - [ ] The centroid in the first iteration is placed randomly
  - [ ] A good cluster are clusters with low `withinss` and high `betweenss`
  - [ ] Cluster with low `withinss` means the character of the data within 1 cluster are similar to each other
  - [ ] The greater the value of `betweenss`, indicates the greater the data variance in each cluster

## 2.2 Building Cluster

Once you find the optimum K from the previous section, try to do K-means clustering from your data and store it as `fruits_cluster`. Use `set.seed(101)` to guarantee a reproducible example. Extract the cluster information from the resulting K-means object using `fruits_cluster$cluster` and add them as a new column named `cluster` to the fruits dataset.

```{r}
RNGkind(sample.kind = "Rounding")
set.seed(101)
# your code here

```

## 2.3 Clusters Profiling

You can use the following chunk to answer question number 7.

```{r}
# your code here

```

7. For a customer who like Apricot, which of the following fruit may be characteristically similar enough to warrant a recommendation?
  - [ ] Kumquat
  - [ ] Banana
  - [ ] Apple

Recall how we can perform a cluster profiling by using a combination of `group_by()` and `summarise_all()`, grouped by the previously created cluster column. After you extract the characteristics for each cluster, try to answer the following question:

```{r}
# your code here

```

8. Can you describe the characteristic fruits that in the same cluster as Mango!
  - [ ] Lowest mean of protein, highest mean of vitamin E, and highest mean of sugars
  - [ ] Lowest mean of protein, highest mean of fiber, and lowest mean of iron_mg
  - [ ] Lowest mean of protein, highest mean of water, and highest mean of vitamin_a
  - [ ] Lowest mean of protein, lowest mean of calcium_mg, and highest mean of sugars